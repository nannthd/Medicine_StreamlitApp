{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaf17K3jROviXT3JGeQJ0B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/Medicine_StreamlitApp/blob/main/app_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syVjQrTcEM5r"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('https://github.com/nannthd/Medicine_StreamlitApp/blob/main/model.pt')\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "collection_name = \"vector_CLIP\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "def detect_and_crop(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                return im_crop\n",
        "\n",
        "    return None\n",
        "\n",
        "def apply_conditions_and_display(image_path):\n",
        "    cropped_image = detect_and_crop(image_path)\n",
        "    if cropped_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "    class_name = top_1.payload.get('class', 'Unknown')\n",
        "\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            prediction = f\"Class Name: {class_name}, Score 1: {score_1:.4f}, Score 2: {score_2:.4f}\"\n",
        "        else:\n",
        "            prediction = f\"Top 1 and top 2 scores are close (Score 1: {score_1:.4f}, Score 2: {score_2:.4f}). Please retake the image.\"\n",
        "\n",
        "    elif 0.85 < score_1 < 0.9:\n",
        "        prediction = f\"This drug might be in the class. Score 1: {score_1:.4f}. Please retake the image.\"\n",
        "\n",
        "    else:\n",
        "        prediction = f\"This drug is not in the class. Score 1: {score_1:.4f}. Saving as a new class.\"\n",
        "\n",
        "    st.image(cropped_image, caption=prediction, use_column_width=True)\n",
        "    st.write(prediction)\n",
        "\n",
        "st.title('Drug Classification and Similarity Search')\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n",
        "if uploaded_file is not None:\n",
        "    apply_conditions_and_display(uploaded_file)"
      ]
    }
  ]
}